from .metrics import MetricsCalculator
from ..models.base_model import BaseModel
from typing import Dict, List
import pandas as pd
import numpy as np

class ModelEvaluator:
    """
    √âvalue les performances des mod√®les sur diff√©rents ensembles de donn√©es.
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.results = {}
    
    def evaluate_model(self, model: BaseModel, X, y, dataset_name='test'):
        """
        √âvalue un mod√®le sur un ensemble de donn√©es.
        
        Args:
            model: mod√®le √† √©valuer
            X, y: donn√©es de test
            dataset_name: nom de l'ensemble ('train', 'valid', 'test')
        
        Returns:
            Dict: m√©triques d'√©valuation
        """
        if not model.is_fitted:
            raise ValueError(f"Le mod√®le {model.name} n'est pas entra√Æn√©.")
        
        # Pr√©dictions
        y_pred = model.predict(X)
        
        # Probabilit√©s (si classification)
        y_pred_proba = None
        if model.task_type == 'classification':
            try:
                y_pred_proba = model.predict_proba(X)
            except:
                pass
        
        # Calculer les m√©triques
        metrics = MetricsCalculator.compute_metrics(
            y, y_pred, model.task_type, y_pred_proba
        )
        
        # Stocker les r√©sultats
        result = {
            'model_name': model.name,
            'task_type': model.task_type,
            'dataset': dataset_name,
            'metrics': metrics,
            'y_true': y,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        key = f"{model.name}_{dataset_name}"
        self.results[key] = result
        
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"üìä √âvaluation de {model.name} sur {dataset_name}")
            print(f"{'='*60}")
            self._print_metrics(metrics, model.task_type)
        
        return result
    
    def evaluate_all(self, models_dict: Dict[str, BaseModel], 
                    X_test, y_test, X_valid=None, y_valid=None):
        """
        √âvalue tous les mod√®les sur les ensembles de test (et validation).
        
        Args:
            models_dict: Dict[str, BaseModel]
            X_test, y_test: donn√©es de test
            X_valid, y_valid: donn√©es de validation (optionnel)
        
        Returns:
            Dict: r√©sultats pour tous les mod√®les
        """
        all_results = {}
        
        for name, model in models_dict.items():
            try:
                # √âvaluation sur test
                test_result = self.evaluate_model(model, X_test, y_test, 'test')
                all_results[f"{name}_test"] = test_result
                
                # √âvaluation sur validation si fourni
                if X_valid is not None and y_valid is not None:
                    valid_result = self.evaluate_model(model, X_valid, y_valid, 'valid')
                    all_results[f"{name}_valid"] = valid_result
                
            except Exception as e:
                if self.verbose:
                    print(f"‚úó Erreur lors de l'√©valuation de {name}: {e}")
                continue
        
        self.results.update(all_results)
        return all_results
    
    def get_comparison_table(self, dataset='test'):
        """
        Cr√©e un tableau comparatif des performances.
        
        Args:
            dataset: 'test' ou 'valid'
        
        Returns:
            pd.DataFrame: tableau comparatif
        """
        comparison = []
        
        for key, result in self.results.items():
            if dataset in key:
                metrics = result['metrics']
                task_type = result['task_type']
                
                if task_type == 'classification':
                    row = {
                        'Model': result['model_name'],
                        'Accuracy': metrics.get('accuracy', 0),
                        'Precision': metrics.get('precision', 0),
                        'Recall': metrics.get('recall', 0),
                        'F1-Score': metrics.get('f1_score', 0),
                        'ROC-AUC': metrics.get('roc_auc', 0) or 0
                    }
                else:  # regression
                    row = {
                        'Model': result['model_name'],
                        'R¬≤': metrics.get('r2_score', 0),
                        'RMSE': metrics.get('rmse', 0),
                        'MAE': metrics.get('mae', 0),
                        'MAPE': metrics.get('mape', 0) or 0
                    }
                
                comparison.append(row)
        
        df = pd.DataFrame(comparison)
        
        # Trier par m√©trique principale
        if not df.empty:
            if 'Accuracy' in df.columns:
                df = df.sort_values('Accuracy', ascending=False)
            elif 'R¬≤' in df.columns:
                df = df.sort_values('R¬≤', ascending=False)
        
        return df
    
    def _print_metrics(self, metrics, task_type):
        """Affiche les m√©triques de mani√®re format√©e."""
        table = MetricsCalculator.format_metrics_table(metrics, task_type)
        print(table.to_string())
    
    def save_results(self, filepath):
        """Sauvegarde les r√©sultats d'√©valuation."""
        import joblib
        joblib.dump(self.results, filepath)
        if self.verbose:
            print(f"‚úì R√©sultats sauvegard√©s: {filepath}")
